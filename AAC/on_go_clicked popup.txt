         # -----------------------------------------------------
    # Go Button (Category → Label → Exact → Tokenized pictograms)
    # -----------------------------------------------------
       # -----------------------------------------------------
    # Go Button: Unified and Stable AAC Input Pipeline
    # -----------------------------------------------------
    def on_go_clicked(self, _=None):
        """
        Final precise AAC 'Go' logic (as per Betsybha's intended rule)
        - Single-word: category → label → fuzzy category → AI → DALL·E
        - Multi-word: sentence → AI-normalized sentence → DALL·E
        """
        import os, csv, re, tempfile, base64
        from difflib import get_close_matches, SequenceMatcher
        start_time = time.time()
        text = (self.text_input.text or "").strip().lower()
        if not text:
            return

        words = text.split()
        is_single_word = len(words) == 1
        self.result_grid.clear_widgets()

            # ==========================================================
        # STEP 1 — Load dataset (now case-insensitive)
        # ==========================================================
        dataset_path = os.path.join(os.path.dirname(__file__), "../dataset.csv")
        dataset_rows = []
        try:
            with open(dataset_path, "r", encoding="utf-8") as f:
                import csv
                reader = csv.DictReader(f)
                # normalize case for all relevant fields
                for r in reader:
                    cat = (r.get("category") or "").strip().lower()
                    lbl = (r.get("label") or "").strip().lower()
                    sent = (r.get("yes_sentence") or "").strip()
                    if sent:
                        dataset_rows.append({
                            "category": cat,
                            "label": lbl,
                            "yes_sentence": sent
                        })
        except Exception as e:
            print("[DATASET LOAD ERROR]", e)
            return

        # collect lowercased sets for quick match
        categories = {r["category"] for r in dataset_rows if r["category"]}
        labels = {r["label"] for r in dataset_rows if r["label"]}

        # user input always in lowercase
        text = (self.text_input.text or "").strip().lower()
        words = text.split()
        is_single_word = len(words) == 1


        def show_category_sentences(category_name):
            """Helper: display all sentences belonging to a category."""
            entry_match = [r for r in dataset_rows if r.get("category", "").lower() == category_name]
            not_entry_match = [r for r in dataset_rows if r.get("category", "").lower() != category_name]
            entries=entry_match+not_entry_match
            self.sug_list.clear_widgets()
            for r in entries:
                sentence = r.get("yes_sentence", "").strip()
                label = r.get("label", "").strip().lower()
                if not sentence:
                    continue
                _, path = self.arasaac_func.get_cached_image(label, PLACEHOLDER_PATH)
                if not path:
                    url = self.arasaac_func.get_arasaac_image_url(label)
                    path = url if url else PLACEHOLDER_PATH
                pictos = [(label, path)]
                row = SuggestionRow(sentence.capitalize(),
                                    ok_callback=self._accept_sentence,
                                    bg_color=(0.8, 1, 0.8, 1))
                row.set_pictos(pictos)
                self.sug_list.add_widget(row)
            self.content_area.clear_widgets()
            self.content_area.add_widget(self.sug_scroll)
            self.back_btn.opacity, self.back_btn.disabled = 1, False
            print(f"[UI] Displayed {len(entries)} sentences for category '{category_name}'")

                # ==========================================================
            # ==========================================================
        # CASE 1 — SINGLE-WORD INPUT (finalized, case-insensitive, AI-safe)
        # ==========================================================
        if is_single_word:
            word = text.strip().lower()
            matched_category = None
            matched_label = None

            # ---------- Normalize dataset categories and labels ----------
            normalized_categories = {c.strip().lower() for c in categories if c}
            normalized_labels = {l.strip().lower() for l in labels if l}

            # ---------- 1️⃣ Exact category match (case-insensitive) ----------
            if word in normalized_categories:
                matched_category = word
                print(f"[CATEGORY MATCH] '{word}' (case-insensitive)")
                entries = [r for r in dataset_rows if r["category"].strip().lower() == matched_category]
                self.sug_list.clear_widgets()
                for r in entries:
                    sentence = r["yes_sentence"].strip()
                    label = r["label"].strip().lower()
                    _, path = self.arasaac_func.get_cached_image(label, PLACEHOLDER_PATH)
                    if not path:
                        url = self.arasaac_func.get_arasaac_image_url(label)
                        path = url if url else PLACEHOLDER_PATH
                    pictos = [(label, path)]
                    row = SuggestionRow(sentence.capitalize(),
                                        ok_callback=self._accept_sentence,
                                        bg_color=(0.8, 1, 0.8, 1))
                    row.set_pictos(pictos)
                    self.sug_list.add_widget(row)
                self.content_area.clear_widgets()
                self.content_area.add_widget(self.sug_scroll)
                self.back_btn.opacity, self.back_btn.disabled = 1, False
                print(f"[UI] Displayed {len(entries)} sentences for category '{matched_category}'")
                return  # ✅ stop here — do NOT go to AI normalization

            # ---------- 2️⃣ Exact label match (case-insensitive) ----------
            if word in normalized_labels:
                for row in dataset_rows:
                    if row["label"].strip().lower() == word:
                        matched_category = row["category"].strip().lower()
                        matched_label = word
                        break
                if matched_category:
                    print(f"[LABEL MATCH] '{word}' → Category '{matched_category}' (case-insensitive)")
                    entr_match = [r for r in dataset_rows if (r["category"].strip().lower() == matched_category and r["label"].strip().lower() == matched_label)]
                    not_entry = [r for r in dataset_rows if( r["category"].strip().lower() == matched_category and r["label"].strip().lower() != matched_label)]
                    entries=entr_match+not_entry
                    self.sug_list.clear_widgets()
                    for r in entries:
                        sentence = r["yes_sentence"].strip()
                        label = r["label"].strip().lower()
                        _, path = self.arasaac_func.get_cached_image(label, PLACEHOLDER_PATH)
                        if not path:
                            url = self.arasaac_func.get_arasaac_image_url(label)
                            path = url if url else PLACEHOLDER_PATH
                        pictos = [(label, path)]
                        row = SuggestionRow(sentence.capitalize(),
                                            ok_callback=self._accept_sentence,
                                            bg_color=(0.8, 1, 0.8, 1))
                        row.set_pictos(pictos)
                        self.sug_list.add_widget(row)
                    self.content_area.clear_widgets()
                    self.content_area.add_widget(self.sug_scroll)
                    self.back_btn.opacity, self.back_btn.disabled = 1, False
                    print(f"[UI] Displayed {len(entries)} sentences for label '{matched_label}'")
                    return  # ✅ stop here — do NOT go to AI normalization

            # ---------- 3️⃣ Fuzzy category match (handles plurals) ----------
            close_cat = get_close_matches(word, list(normalized_categories), n=1, cutoff=0.8)
            if not close_cat and word.endswith("s"):
                singular = word[:-1]
                close_cat = get_close_matches(singular, list(normalized_categories), n=1, cutoff=0.8)
            if close_cat:
                matched_category = close_cat[0]
                print(f"[FUZZY CATEGORY MATCH] '{word}' ≈ '{matched_category}'")
                entries = [r for r in dataset_rows if r["category"].strip().lower() == matched_category]
                self.sug_list.clear_widgets()
                for r in entries:
                    sentence = r["yes_sentence"].strip()
                    label = r["label"].strip().lower()
                    _, path = self.arasaac_func.get_cached_image(label, PLACEHOLDER_PATH)
                    if not path:
                        url = self.arasaac_func.get_arasaac_image_url(label)
                        path = url if url else PLACEHOLDER_PATH
                    pictos = [(label, path)]
                    row = SuggestionRow(sentence.capitalize(),
                                        ok_callback=self._accept_sentence,
                                        bg_color=(0.8, 1, 0.8, 1))
                    row.set_pictos(pictos)
                    self.sug_list.add_widget(row)
                self.content_area.clear_widgets()
                self.content_area.add_widget(self.sug_scroll)
                self.back_btn.opacity, self.back_btn.disabled = 1, False
                print(f"[UI] Displayed {len(entries)} sentences for fuzzy category '{matched_category}'")
                return  # ✅ stop here — do NOT go to AI normalization
            def  word_dictionary(word):
                """Check if word exists in WordNet dictionary."""
                from nltk.corpus import wordnet
                word=word.lower()
                return bool(wordnet.synsets(word))
            if word_dictionary(word):
                print(f"[DICTIONARY CHECK] '{word}' found in WordNet dictionary.")
                norm_word = word
            if not word_dictionary(word):
                print(f"[DICTIONARY CHECK] '{word}' NOT found in WordNet dictionary.")
            # ---------- 4️⃣ AI normalization (only if nothing matched) ----------
                print(f"[AI NORMALIZATION - 1 word] Triggered for '{word}' (no category/label match)")
                try:
                    norm_word = ai_normalize_input(
                        word,
                        location=self.location_label,
                        time_phase=getattr(self, "time_phase", "unspecified"),
                    ).lower().strip()
                    import re
                    norm_word = re.sub(r'["\'\.\,\;\:\!\?]', '', norm_word)  # Remove quotes, periods, punctuation
                    norm_word = re.sub(r'\s+', ' ', norm_word).strip()        # Remove extra spaces
                    print(f"[AI CLEANED] '{word}' → '{norm_word}'")
                    self.text_input.text = norm_word  # Update input box with normalized word
                except Exception as e:
                    print("[AI NORMALIZER ERROR]", e)
                    norm_word = word
                    import re
                    norm_word = re.sub(r'["\'\.\,\;\:\!\?]', '', norm_word)
                    norm_word = re.sub(r'\s+', ' ', norm_word).strip()
                if norm_word != word:
                    print(f"[AI→CORRECTED] '{word}' → '{norm_word}'")
                    # re-run checks for corrected word
                    if norm_word in normalized_categories:
                        print(f"[AI→CATEGORY MATCH] '{norm_word}'")
                        entries = [r for r in dataset_rows if r["category"].strip().lower() == norm_word]
                        self.sug_list.clear_widgets()
                        for r in entries:
                            sentence = r["yes_sentence"].strip()
                            label = r["label"].strip().lower()
                            _, path = self.arasaac_func.get_cached_image(label, PLACEHOLDER_PATH)
                            if not path:
                                url = self.arasaac_func.get_arasaac_image_url(label)
                                path = url if url else PLACEHOLDER_PATH
                            pictos = [(label, path)]
                            row = SuggestionRow(sentence.capitalize(),
                                                ok_callback=self._accept_sentence,
                                                bg_color=(0.8, 1, 0.8, 1))
                            row.set_pictos(pictos)
                            self.sug_list.add_widget(row)
                        self.content_area.clear_widgets()
                        self.content_area.add_widget(self.sug_scroll)
                        self.back_btn.opacity, self.back_btn.disabled = 1, False
                        print(f"[UI] Displayed {len(entries)} sentences for corrected category '{norm_word}'")
                        return

                    if norm_word in normalized_labels:
                        for row in dataset_rows:
                            if row["label"].strip().lower() == norm_word:
                                matched_category = row["category"].strip().lower()
                                break
                        if matched_category:
                            print(f"[AI→LABEL MATCH] '{norm_word}' → Category '{matched_category}'")
                            first_entry = [r for r in dataset_rows if (r["category"].strip().lower() == matched_category and r["label"].strip().lower() == norm_word)]
                            other_entry = [r for r in dataset_rows if (r["category"].strip().lower() == matched_category and r["label"].strip().lower() != norm_word)]
                            entries = first_entry + other_entry
                            self.sug_list.clear_widgets()
                            for r in entries:
                                sentence = r["yes_sentence"].strip()
                                label = r["label"].strip().lower()
                                _, path = self.arasaac_func.get_cached_image(label, PLACEHOLDER_PATH)
                                if not path:
                                    url = self.arasaac_func.get_arasaac_image_url(label)
                                    path = url if url else PLACEHOLDER_PATH
                                pictos = [(label, path)]
                                row = SuggestionRow(sentence.capitalize(),
                                                    ok_callback=self._accept_sentence,
                                                    bg_color=(0.8, 1, 0.8, 1))
                                row.set_pictos(pictos)
                                self.sug_list.add_widget(row)
                            self.content_area.clear_widgets()
                            self.content_area.add_widget(self.sug_scroll)
                            self.back_btn.opacity, self.back_btn.disabled = 1, False
                            print(f"[UI] Displayed {len(entries)} sentences for corrected label '{norm_word}'")
                            return

            # ---------- 5️⃣ DALL·E fallback ----------
            print(f"[DALL·E] No dataset match even after AI normalization → generating image for '{norm_word}'")
            try:
                from openai import OpenAI
                client = OpenAI(api_key="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")
                dalle_prompt = (
                    f"3D cartoon icon, , pictogram style, "
                    f"no text, minimal, high contrast; concept: '{norm_word}'."
                )
                resp = client.images.generate(
                    model="dall-e-3",
                    prompt=dalle_prompt,
                    size="1024x1024",
                    response_format="b64_json",
                    n=1,
                )
                b64 = resp.data[0].b64_json
                safe = re.sub(r"[^a-z0-9_]+", "_", norm_word)
                out_path = os.path.join(tempfile.gettempdir(), f"dalle_{safe}.png")
                # After image generation:
                # out_path = ... (local temp path for DALL·E image)
                #self.prompt_category_popup(label=word, img_path=out_path, sentence=text)

                with open(out_path, "wb") as f:
                    f.write(base64.b64decode(b64))
                self.show_add_button_for_dalle_result(label=norm_word, img_path=out_path, sentence=norm_word)

                return
            except Exception as e:
                print("[DALL·E ERROR]", e)
                return




        # ==========================================================
        # CASE 2 — MULTI-WORD INPUT
        # ==========================================================
        matched_sentence = None
        matched_label = None
        matched_category = None

        best_score = 0.0
        for row in dataset_rows:
            sent = (row.get("yes_sentence") or "").strip().lower()
            score = SequenceMatcher(None, text, sent).ratio()
            if score > best_score:
                best_score = score
                matched_sentence = sent
                matched_label = (row.get("label") or "").lower()
                matched_category = (row.get("category") or "").lower()
        # ==========================================================
        # OPTIONAL TF–IDF SENTENCE SIMILARITY (ADDITIVE LOGIC)
        # ==========================================================
        try:
            # Collect all sentences from dataset
            sentences = [r.get("yes_sentence", "").strip().lower() for r in dataset_rows if r.get("yes_sentence")]
            if sentences:
                # Fit TF–IDF on all dataset sentences
                vectorizer = TfidfVectorizer()
                tfidf_matrix = vectorizer.fit_transform(sentences)

                # Transform user input sentence
                tfidf_input = vectorizer.transform([text])

                # Compute cosine similarity
                cosine_scores = cosine_similarity(tfidf_input, tfidf_matrix)[0]

                # Find the best match
                best_tfidf_idx = cosine_scores.argmax()
                best_tfidf_score = cosine_scores[best_tfidf_idx]
                best_tfidf_sentence = sentences[best_tfidf_idx]

                # If TF–IDF finds a stronger match than SequenceMatcher
                if best_tfidf_score > best_score:
                    print(f"[TF–IDF MATCH] '{best_tfidf_sentence}' (score={best_tfidf_score:.2f})")
                    matched_sentence = best_tfidf_sentence
                    matched_label = (dataset_rows[best_tfidf_idx].get("label") or "").lower()
                    matched_category = (dataset_rows[best_tfidf_idx].get("category") or "").lower()
                    best_score = best_tfidf_score
        except Exception as e:
            print(f"[TF–IDF ERROR] {e}")

        if best_score >= 0.85:
            print(f"[SENTENCE MATCH] '{matched_sentence}' (score={best_score:.2f})")
            _, path = self.arasaac_func.get_cached_image(matched_label, PLACEHOLDER_PATH)
            if not path:
                url = self.arasaac_func.get_arasaac_image_url(matched_label)
                path = url if url else PLACEHOLDER_PATH
            widget = ClickableImage(label_text=matched_label.capitalize(), img_url=path,
                                    on_release=lambda *_: self.speak_text(matched_sentence))
            self.result_grid.add_widget(widget)
            show_category_sentences(matched_category)
            return
        

        # --- AI normalization for multi-word ---
        print(f"[AI NORMALIZATION] Correcting multi-word sentence '{text}'")
        try:
            norm_text = ai_normalize_input(
                text,
                location=self.location_label,
                time_phase=getattr(self, "time_phase", "unspecified"),
            ).lower().strip()
            self.text_input.text = norm_text
        except Exception as e:
            print("[AI NORMALIZER ERROR]", e)
            norm_text = text

        best_score = 0.0
        for row in dataset_rows:
            sent = (row.get("yes_sentence") or "").strip().lower()
            score = SequenceMatcher(None, norm_text, sent).ratio()
            if score > best_score:
                best_score = score
                matched_sentence = sent
                matched_label = (row.get("label") or "").lower()
                matched_category = (row.get("category") or "").lower()

        if best_score >= 0.85:
            print(f"[AI→MATCH] '{matched_sentence}' (score={best_score:.2f})")
            _, path = self.arasaac_func.get_cached_image(matched_label, PLACEHOLDER_PATH)
            if not path:
                url = self.arasaac_func.get_arasaac_image_url(matched_label)
                path = url if url else PLACEHOLDER_PATH
            widget = ClickableImage(label_text=matched_label.capitalize(), img_url=path,
                                    on_release=lambda *_: self.speak_text(matched_sentence))
            self.result_grid.add_widget(widget)
            show_category_sentences(matched_category)
            return

        # --- DALL·E fallback ---
        print(f"[DALL·E] No dataset match after AI normalization → generating image for '{norm_text}'")
        try:
            from openai import OpenAI
            client = OpenAI(api_key="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")
            dalle_prompt = (
                f"3D colorfull icon,pictogram style, "
                f"no text, minimal, high contrast; concept: generate an cartoon 3d icon for '{norm_text}'."
            )
            resp = client.images.generate(
                model="dall-e-3",
                prompt=dalle_prompt,
                size="1024x1024",
                response_format="b64_json",
                n=1,
            )
            b64 = resp.data[0].b64_json
            safe = re.sub(r"[^a-z0-9_]+", "_", norm_text)
            out_path = os.path.join(tempfile.gettempdir(), f"dalle_{safe}.png")
            with open(out_path, "wb") as f:
                f.write(base64.b64decode(b64))
            self.show_add_button_for_dalle_result(label=norm_text, img_path=out_path, sentence=norm_text)


            return
        except Exception as e:
            print("[DALL·E ERROR]", e)
            return